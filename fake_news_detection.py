# -*- coding: utf-8 -*-
"""FAKE_NEWS_DETECTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2QYCfEIjj0YA43GtUYj3dFujDr76TEj
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
import optuna
import itertools
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification
from sklearn.metrics import make_scorer, f1_score
from sklearn.metrics import accuracy_score, precision_score, recall_score

mpl.style.use("seaborn-v0_8-deep")
mpl.rcParams["figure.figsize"] = (20, 5)
mpl.rcParams["figure.dpi"] = 100
plt.rcParams["figure.dpi"] = 100
plt.rcParams["lines.linewidth"] = 2

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.sentiment import SentimentIntensityAnalyzer
from collections import Counter
import re
from wordcloud import WordCloud

import xgboost as xgb
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import itertools

nltk.download("vader_lexicon")
nltk.download("punkt")
nltk.download("stopwords")

import pandas as pd

# Try reading the file with 'latin-1' encoding
data_fake = pd.read_csv('news.csv', on_bad_lines='skip', quoting=3, encoding='latin-1')

# If the above doesn't work, try 'ISO-8859-1'
# data_fake = pd.read_csv('news.csv', on_bad_lines='skip', quoting=3, encoding='ISO-8859-1')

def pre_process(df):
    columns = ["title", "text"]

    for col in columns:
        df[col] = df[col].astype(str)  # Convert each column to string

    # Map 'label' values from 'FAKE'/'REAL' to 1/0
    df["label"] = df["label"].map({"FAKE": 1, "REAL": 0}) # Correct the indentation here

    # Remove the 'Unnamed: 0' column, if it exists, as it's usually an artifact
    df = df.drop(["Unnamed: 0"], axis=1)

    # Split the DataFrame into two: one with 'title' and 'label', and one with 'text' and 'label'
    df_title = df[["title", "label"]]
    df_text = df[["text", "label"]]

    # Define the directory where you want to save the pickle files
    directory_path = "D:\\Projects\\Fake News Detection\\Data\\"

    # Export df_title as a pickle file
    # df_title.to_pickle(directory_path + "df_title.pkl")

    # Export df_text as a pickle file
    # df_text.to_pickle(directory_path + "df_text.pkl")

    print("DataFrames exported as pickle files successfully.")

    return df_title, df_text

def num_characters(df, col_name):
    df["Characters"] = df[col_name].apply(len)
    return df


def num_sentences(df, col_name):
    df["Sentences"] = df[col_name].apply(lambda x: len(nltk.sent_tokenize(x)))
    return df # Fixed indentation


def num_words(df, col_name):
    df["Words"] = df[col_name].apply(lambda x: len(nltk.word_tokenize(x)))
    return df # Fixed indentation

def pie_plot(df, col_name):
    sizes = df["label"].value_counts()
    labels = ["Real", "Fake"]  # Corrected indentation
    colors = ["#ff9999", "#66b3ff"]
    plt.pie(
        sizes,
        autopct="%1.1f%%",
        colors=colors,
        startangle=90,
        explode=(0.1, 0),
        shadow=True,
    )

    plt.title("Distribution of Real and Fake News in " + col_name)
    plt.legend(labels, title="News Type", loc="center left", bbox_to_anchor=(1, 0.5))
    plt.show()

def plot_distribution_features(df, features, col_name):
    fig, axes = plt.subplots(
        nrows=len(features), ncols=1, figsize=(15, 6 * len(features))
    )

    for i, feature in enumerate(features): # Fixed indentation - removed extra space
        # Plot distribution for 'Real' news
        sns.histplot(
            df[df["label"] == 0][feature],
            kde=True,
            color="#66b3ff",
            label="Real",
            ax=axes[i],
        )
        # Plot distribution for 'Fake' news
        sns.histplot(
            df[df["label"] == 1][feature],
            kde=True,
            color="#ff9999",
            label="Fake",
            ax=axes[i],
        )
        # Setting the title for each subplot
        axes[i].set_title(f"{feature} Distribution for {col_name} dataset")
        # Setting the labels for each subplot
        axes[i].set_xlabel(feature)
        axes[i].set_ylabel("Count")
        # Adding legend to each subplot
        axes[i].legend()

    # Adjust layout
    plt.tight_layout()
    plt.show()

def plot_mean_median(df, features, col_name):
    for feature in features:
        df[feature] = pd.to_numeric(df[feature], errors="coerce")

    # Create a figure with subplots
    fig, axes = plt.subplots(
        nrows=len(features), ncols=1, figsize=(10, 5 * len(features))
    )

    # Check if we have more than one feature to plot, if not, wrap axes in a list
    if len(features) == 1:
        axes = [axes]

    for i, feature in enumerate(features):
        # Prepare a DataFrame with mean and median values for both labels
        summary_df = (
            df.groupby("label")[feature]
            .agg(["mean", "median"])
            .reset_index()
            .melt(id_vars="label")
        )

        # Plot
        sns.barplot(
            x="label",
            y="value",
            hue="variable",
            data=summary_df,
            ax=axes[i],
            palette="coolwarm",
        )
        axes[i].set_title(f"Mean and Median of {feature} for {col_name}")
        axes[i].set_xlabel("Label")
        axes[i].set_ylabel(f"{feature} Count")
        axes[i].legend(title="Statistic")

    plt.tight_layout()
    plt.show()

def process_text(text):
    text = text.lower()

    # Tokenization
    tokens = word_tokenize(text)  # Indented to align with other lines in the function

    # Removing special characters
    tokens = [re.sub(r"[^a-zA-Z0-9]", "", token) for token in tokens]
    tokens = [token for token in tokens if token]  # Remove empty strings

    # Removing stop words and punctuation
    stop_words = set(stopwords.words("english"))
    tokens = [token for token in tokens if token not in stop_words]

    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(token) for token in tokens]

    return tokens

def analyze_sentiment(text):
    if not isinstance(text, str):
        raise ValueError("Input must be a string")

    sia = SentimentIntensityAnalyzer()
    score = sia.polarity_scores(text)

    if score["compound"] >= 0.05:
        return "positive"
    elif score["compound"] <= -0.05:
        return "negative"
    else:
        return "neutral"

data_fake

data_fake_title, data_fake_text = pre_process(data_fake)

data_fake_title

data_fake_title = num_characters(data_fake_title, "title")
data_fake_title = num_sentences(data_fake_title, "title")
data_fake_title = num_words(data_fake_title, "title")

data_fake_text = num_characters(data_fake_text, "text")
data_fake_text = num_sentences(data_fake_text, "text")
data_fake_text = num_words(data_fake_text, "text")

data_fake_title

data_fake_text

pie_plot(data_fake_title, "Title")

pie_plot(data_fake_text, "Text")

plot_distribution_features(data_fake_title, ["Characters", "Sentences", "Words"], "title")

plot_distribution_features(data_fake_text, ["Characters", "Sentences", "Words"], "text")

plot_mean_median(data_fake_title, ["Characters", "Sentences", "Words"], "title")

plot_mean_median(data_fake_text, ["Characters", "Sentences", "Words"], "text")

data_fake_title["Processed"] = data_fake_title["title"].apply(process_text)

data_fake_text["Processed"] = data_fake_text["text"].apply(process_text)

data_fake_text

data_fake_title

data_fake_title["Sentiment"] = data_fake_title["title"].apply(analyze_sentiment)

data_fake_text["Sentiment"] = data_fake_text["text"].apply(analyze_sentiment)

plt.pie(
    data_fake_title[data_fake_title["label"] == 0]["Sentiment"].value_counts(),
    autopct="%1.1f%%",
    colors=["#99ff99", "#66b3ff", "#ff9999"],
    startangle=90,
    explode=(0.1, 0, 0),
    shadow=True,
)
plt.legend(labels=["Neutral", "Positive", "Negative"])
plt.title("Sentiment Distribution for Real News in Title")
plt.show()
plt.clf()
plt.pie(
    data_fake_title[data_fake_title["label"] == 1]["Sentiment"].value_counts(),
    autopct="%1.1f%%",
    colors=["#99ff99", "#66b3ff", "#ff9999"],
    startangle=90,
    explode=(0.1, 0, 0),
    shadow=True,
)
plt.legend(labels=["Neutral", "Positive", "Negative"])
plt.title("Sentiment Distribution for Fake News in Title")
plt.show()

plt.pie(
    data_fake_text[data_fake_text["label"] == 0]["Sentiment"].value_counts(),
    autopct="%1.1f%%",
    colors=["#99ff99", "#66b3ff", "#ff9999"],
    startangle=90,
    explode=(0.1, 0, 0),
    shadow=True,
)
plt.legend(labels=["Neutral", "Positive", "Negative"])
plt.title("Sentiment Distribution for Real News in Text")
plt.show()
plt.clf()
plt.pie(
    data_fake_text[data_fake_text["label"] == 1]["Sentiment"].value_counts(),
    autopct="%1.1f%%",
    colors=["#99ff99", "#66b3ff", "#ff9999"],
    startangle=90,
    explode=(0.1, 0, 0),
    shadow=True,
)
plt.legend(labels=["Neutral", "Positive", "Negative"])
plt.title("Sentiment Distribution for Fake News in Text")
plt.show()

sns.set(style="darkgrid", palette="muted", color_codes=True)

positive_reviews = data_fake_title[data_fake_title["Sentiment"] == "positive"]
negative_reviews = data_fake_title[data_fake_title["Sentiment"] == "negative"]

sns.distplot(
    positive_reviews["Words"],
    color="steelblue",
    label="Positive",
    kde=False,
    hist_kws={"alpha": 0.7},
)
sns.distplot(
    negative_reviews["Words"],
    color="tomato",
    label="Negative",
    kde=False,
    hist_kws={"alpha": 0.7},
)

plt.title("Distribution of Word Count by Sentiment")
plt.xlabel("Word Count")
plt.ylabel("Frequency")
plt.legend()

sns.set(style="darkgrid", palette="muted", color_codes=True)

positive_reviews = data_fake_text[data_fake_text["Sentiment"] == "positive"]
negative_reviews = data_fake_text[data_fake_text["Sentiment"] == "negative"]

sns.distplot(
    positive_reviews["Words"],
    color="steelblue",
    label="Positive",
    kde=False,
    hist_kws={"alpha": 0.7},
)
sns.distplot(
    negative_reviews["Words"],
    color="tomato",
    label="Negative",
    kde=False,
    hist_kws={"alpha": 0.7},
)

plt.title("Distribution of Word Count by Sentiment")
plt.xlabel("Word Count")
plt.ylabel("Frequency")
plt.legend()

Real_title = data_fake_text[data_fake_text["label"] == 0]["Processed"].astype(str).str.cat(sep=" ")
Fake_title = data_fake_text[data_fake_text["label"] == 1]["Processed"].astype(str).str.cat(sep=" ")

spam_wc = WordCloud(
    width=800, height=800, background_color="black", stopwords=None, min_font_size=10
).generate(Real_title)

ham_wc = WordCloud(
    width=800, height=800, background_color="white", stopwords=None, min_font_size=10
).generate(Fake_title)

plt.figure(figsize=(15, 7))
plt.subplot(1, 2, 1)
plt.imshow(spam_wc)
plt.title("Real text Word Cloud")
plt.axis("off")
plt.subplot(1, 2, 2)
plt.imshow(ham_wc)
plt.title("Fake text Word Cloud")
plt.axis("off")
plt.show()

Real_text_postive = (
    data_fake_text[(data_fake_text["label"] == 0) & (data_fake_text["Sentiment"] == "positive")]["Processed"]
    .astype(str)
    .str.cat(sep=" ")
)
Real_text_negative = (
    data_fake_text[(data_fake_text["label"] == 0) & (data_fake_text["Sentiment"] == "negative")]["Processed"]
    .astype(str)
    .str.cat(sep=" ")
)
Real_text_neutral = (
    data_fake_text[(data_fake_text["label"] == 0) & (data_fake_text["Sentiment"] == "neutral")]["Processed"]
    .astype(str)
    .str.cat(sep=" ")
)
Real_wc_p = WordCloud(
    width=800, height=800, background_color="blue", stopwords=None, min_font_size=10
).generate(Real_text_postive)

Real_wc_n = WordCloud(
    width=800, height=800, background_color="red", stopwords=None, min_font_size=10
).generate(Real_text_negative)
Real_wc_nu = WordCloud(
    width=800, height=800, background_color="green", stopwords=None, min_font_size=10
).generate(Real_text_neutral)

plt.figure(figsize=(15, 7))
plt.subplot(1, 3, 1)
plt.imshow(Real_wc_p)
plt.title("Real Positive text Word Cloud")
plt.axis("off")
plt.subplot(1, 3, 2)
plt.imshow(Real_wc_n)
plt.title("Real Negative text Word Cloud")
plt.axis("off")
plt.subplot(1, 3, 3)
plt.imshow(Real_wc_nu)
plt.title("Real Neutral text Word Cloud")
plt.axis("off")
plt.show()

Real_text_postive = (
    data_fake_text[(data_fake_text["label"] == 1) & (data_fake_text["Sentiment"] == "positive")]["Processed"]
    .astype(str)
    .str.cat(sep=" ")
)
Real_text_negative = (
    data_fake_text[(data_fake_text["label"] == 1) & (data_fake_text["Sentiment"] == "negative")]["Processed"]
    .astype(str)
    .str.cat(sep=" ")
)
Real_text_neutral = (
    data_fake_text[(data_fake_text["label"] == 1) & (data_fake_text["Sentiment"] == "neutral")]["Processed"]
    .astype(str)
    .str.cat(sep=" ")
)
Real_wc_p = WordCloud(
    width=800, height=800, background_color="blue", stopwords=None, min_font_size=10
).generate(Real_text_postive)

Real_wc_n = WordCloud(
    width=800, height=800, background_color="red", stopwords=None, min_font_size=10
).generate(Real_text_negative)
Real_wc_nu = WordCloud(
    width=800, height=800, background_color="green", stopwords=None, min_font_size=10
).generate(Real_text_neutral)

plt.figure(figsize=(15, 7))
plt.subplot(1, 3, 1)
plt.imshow(Real_wc_p)
plt.title("Fake Positive text Word Cloud")
plt.axis("off")
plt.subplot(1, 3, 2)
plt.imshow(Real_wc_n)
plt.title("Fake Negative text Word Cloud")
plt.axis("off")
plt.subplot(1, 3, 3)
plt.imshow(Real_wc_nu)
plt.title("Fake Neutral text Word Cloud")
plt.axis("off")
plt.show()

data_fake_title

def optimize_bnb(trial, X_train, X_test, y_train, y_test):
    alpha = trial.suggest_float("alpha", 1e-4, 10.0)
    binarize = trial.suggest_float("binarize", 0.0, 1.0)
    fit_prior = trial.suggest_categorical("fit_prior", [True, False])
    model = BernoulliNB(alpha=alpha, binarize=binarize, fit_prior=fit_prior)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    return accuracy_score(y_test, predictions)

def optimize_xgb(trial, X_train, y_train, X_valid, y_valid):
    params = {
        "verbosity": 0,
        "objective": "binary:logistic",
        "tree_method": "exact",  # Change to 'gpu_hist' for GPU
        "lambda": trial.suggest_loguniform("lambda", 1e-8, 1.0),
        "alpha": trial.suggest_loguniform("alpha", 1e-8, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.2, 1.0),
        "subsample": trial.suggest_float("subsample", 0.2, 1.0),
        "learning_rate": trial.suggest_float("learning_rate", 0.005, 0.5),
        "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 9),
        "min_child_weight": trial.suggest_int("min_child_weight", 2, 10),
    }

    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    accuracy = accuracy_score(y_valid, preds)
    return accuracy

def optimize_xgb(trial, X_train, y_train, X_valid, y_valid):
    params = {
        "verbosity": 0,
        "objective": "binary:logistic",
        "tree_method": "exact",  # Change to 'gpu_hist' for GPU
        "lambda": trial.suggest_loguniform("lambda", 1e-8, 1.0),
        "alpha": trial.suggest_loguniform("alpha", 1e-8, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.2, 1.0),
        "subsample": trial.suggest_float("subsample", 0.2, 1.0),
        "learning_rate": trial.suggest_float("learning_rate", 0.005, 0.5),
        "n_estimators": trial.suggest_int("n_estimators", 100, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 9),
        "min_child_weight": trial.suggest_int("min_child_weight", 2, 10),
    }

    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    accuracy = accuracy_score(y_valid, preds)
    return accuracy
def plot_confusion_matrix(cm, classes, title="Confusion matrix"):
    plt.figure(figsize=(5, 5))
    plt.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.0
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(
            j,
            i,
            format(cm[i, j], "d"),
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black",
        )

    plt.tight_layout()
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
    plt.grid(False)
    plt.show()

tfidf = TfidfVectorizer(max_features=3000)

X1 = data_fake_title["Processed"].astype(str)
y1 = data_fake_title["label"]
X2 = data_fake_text["Processed"].astype(str)
y2 = data_fake_text["label"]

X1 = tfidf.fit_transform(X1).toarray()
X2 = tfidf.fit_transform(X2).toarray()

X1_train, X1_test, y1_train, y1_test = train_test_split(
    X1, y1, test_size=0.2, random_state=42
)
X2_train, X2_test, y2_train, y2_test = train_test_split(
    X2, y2, test_size=0.2, random_state=42
)

title_model = optuna.create_study(direction="maximize")
title_model.optimize(
    lambda trial: optimize_bnb(trial, X1_train, X1_test, y1_train, y1_test),
    n_trials=100,
)

import optuna.visualization as ov



optimization_history = ov.plot_optimization_history(title_model)


optimization_history.show()

text_model = optuna.create_study(direction="maximize")
text_model.optimize(
    lambda trial: optimize_bnb(trial, X2_train, X2_test, y2_train, y2_test),
    n_trials=100,
)

import optuna.visualization as ov  # Re-import the module in this cell

optimization_history = ov.plot_optimization_history(text_model)
optimization_history.show()

y2_train

XGb_model = optuna.create_study(direction="maximize")
XGb_model.optimize(
    lambda trial: optimize_xgb(trial, X2_train, y2_train, X2_test, y2_test),
    n_trials=25,
)

optimization_history = ov.plot_optimization_history(XGb_model)
optimization_history.show()

best_params = XGb_model.best_params

best_params = XGb_model.best_params
XGb_model = xgb.XGBClassifier(**best_params)
XGb_model.fit(X2_train, y2_train)
y2_pred = XGb_model.predict(X2_test)
print("Accuracy: ", accuracy_score(y2_test, y2_pred))
print("Precision: ", precision_score(y2_test, y2_pred))

cm = confusion_matrix(y2_test, y2_pred)
classes = ["Real", "Fake"]
plot_confusion_matrix(cm, classes)
print("Confusion Matrix XgBoost: ", cm)

best_params = title_model.best_params

model = BernoulliNB(
    alpha=best_params["alpha"],
    binarize=best_params["binarize"],
    fit_prior=best_params["fit_prior"],
)

model.fit(X1_train, y1_train)


y_pred = model.predict(X1_test)
print("Accuracy: ", accuracy_score(y1_test, y_pred))
print("Precision: ", precision_score(y1_test, y_pred))

cm = confusion_matrix(y1_test, y_pred)
classes = ["Real", "Fake"]
plot_confusion_matrix(cm, classes)

best_params = text_model.best_params

model_text = BernoulliNB(
    alpha=best_params["alpha"],
    binarize=best_params["binarize"],
    fit_prior=best_params["fit_prior"],
)

model_text.fit(X2_train, y2_train)


y_pred = model_text.predict(X2_test)
print("Accuracy: ", accuracy_score(y2_test, y_pred))
print("Precision: ", precision_score(y2_test, y_pred))

cm = confusion_matrix(y2_test, y_pred)
classes = ["Real", "Fake"]
plot_confusion_matrix(cm, classes)

pickle.dump(XGb_model, open("Xgb_model.pkl", "wb"))
pickle.dump(model, open("title_model.pkl", "wb"))

