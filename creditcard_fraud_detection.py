# -*- coding: utf-8 -*-
"""creditcard_fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FOvEBOrG-HzlcOYb5E9_ajXIgz_OKJr1
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report,confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("creditcard.csv")

df.head()

df.shape

df.info()

df.describe()

df.isnull().sum()

df.Class.value_counts()

ax = df.Class.value_counts().plot(kind='bar')

plt.title("Transaction Class Distribution")
plt.xlabel("Class")
plt.ylabel("Frequncy")

normal = df[df.Class == 0]
fraud = df[df.Class == 1]

print(normal.shape)
print(fraud.shape)

nt = len(normal)/len(df)*100
print(nt)

ft = len(fraud)/len(df)*100
print(ft)

normal.Amount.describe()

fraud.Amount.describe()

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)
f.suptitle('Amount per transaction by class')
bins = 50
ax1.hist(fraud.Amount, bins = bins)
ax1.set_title('Fraud')
ax2.hist(normal.Amount, bins = bins)
ax2.set_title('Normal')
plt.xlabel('Amount ($)')
plt.ylabel('Number of Transactions')
plt.xlim((0, 20000))
plt.yscale('log')
plt.show();

plt.figure(figsize=(20,20))
sns.heatmap(df.corr(),cmap="RdYlGn", annot=True)

ligit = normal.sample(n=492)

new_df  = pd.concat([ligit,fraud], axis = 0)

new_df["Class"].value_counts()

x = new_df.drop(columns= "Class", axis = 1)
y = new_df.Class

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state= 40)

print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)

df.shape

model_params = {
    'logistic regression' : {
        'model': LogisticRegression(), # Now you can use LogisticRegression
        'parameter' : {
            'solver': ['liblinear']
        }
    },
    'svm' : {
        'model' : svm.SVC(),
        'parameter' : {
            'kernel' : ['rbf','linear'],
            'C' : [10,15,20]
        }
    },
    'decision tree' : {
        'model' : DecisionTreeClassifier(),
        'parameter' : {
            'criterion' : ['gini', 'entropy']
        }
    },
    'random forest' : {
        'model': RandomForestClassifier(),
        'parameter' : {
            'criterion': ['gini','entropy'],
            'n_estimators' : [50,100,150]
        }
    },
    'naive_bayes_gaussian' : {
        'model' : GaussianNB(),
        'parameter' : {}
    },
    'k nearest neighbors': {
        'model' : KNeighborsClassifier(),
        'parameter' : {
            'n_neighbors' : [5,10,15]
        }
    }
}

score = []

for model_name, mp in model_params.items():
    clf = GridSearchCV(mp['model'], mp['parameter'], cv=5)
    clf.fit(x_train,y_train)
    score.append({
        'model' : model_name,
        'best_score' : clf.best_score_,
        'best_params' : clf.best_params_
    })

cc_df = pd.DataFrame(score, columns = ['model', 'best_score', 'best_params'])
cc_df

